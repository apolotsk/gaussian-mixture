# Gaussian Mixture: Find Parameters using [Expectation-maximization][expectation-maximization-wiki]

This is an example of finding parameters for a Gaussian Mixture using a mathematical optimization mathod [Expectation-maximization][expectation-maximization-wiki].
- Finds $Œ∏ := \arg\max_Œ∏ L(Œ∏|X)$.
- $L(Œ∏|X)$ is the **expected log-likelihood** of $Œ∏$ given $X$.
- [Converges to local][expectation-maximization-wiki] _maximum likelihood_.
- Is implemented using NumPy.
- Is based on a [lecture from University of Washington course "Foundations of Computing II"][slides].

[expectation-maximization-wiki]: https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm

[slides]: http://courses.cs.washington.edu/courses/cse312/11wi/slides/12em.pdf "University of Washington, Course 'Foundations of Computing II', Lecture '12:	Expectation Maximization'"

## Usage

```
$ python 'Gaussian Mixture.py'
The real probability probability of selecting Gaussian 1 is 0.30 and Gaussian 2 is 0.70.
The real mean of Gaussian 1 is 13.00 and Gaussian 1 is 8.00.
The real standard deviation of Gaussian 1 is 1.00 and Gaussian 1 is 1.40.
The real sample count of Gaussian 1 is 307 and Gaussian 2 is 693.

Finding the parameters given only the samples.
The predicted probability probability of selecting Gaussian 1 is 0.30 and Gaussian 2 is 0.70.
The predicted mean of Gaussian 1 is 13.09 and Gaussian 2 is 7.96.
The predicted standard deviation of Gaussian 1 is 0.97 and Gaussian 2 is 1.36.
The predicted sample count of Gaussian 1 is 304.1 and Gaussian 2 is 695.9.
```

![Histogram of the observable data and the latent data](.README.md/Histogram%20of%20the%20observable%20data%20and%20the%20latent%20data.svg)

![Histogram of the observable data](.README.md/Histogram%20of%20the%20observable%20data.svg)

![Find parameters](.README.md/Find%20parameters.gif)

## The algorithm

1. $Œ∏_0 :=$ random values of the parameters.
2. $t :=$ 0 to $‚àû$:
   1. $L(Œ∏|Œ∏_t,X) := \sum_x p(x) \sum_z{\color{orange}p(z|x,Œ∏_t)}\log {\color{green}p(x,z|Œ∏)}$
      - $L(Œ∏|Œ∏_t,X)$ is an approximation to $-\text{D}_\text{KL}(p(x)||p(x|Œ∏))$.
      - Is called _expectation step_.
      - ${\color{orange}p(z|x,Œ∏_t)} = {\color{darkgreen}p(x,z|Œ∏_t)}/\sum_z {\color{darkgreen}p(x,z|Œ∏_t)}$
      - $p(x) := 1/|X|$.
        - [Because $x$ are independent and identically distributed][mle-wiki].
   2. $Œ∏_{t+1} := \arg\max_Œ∏ L(Œ∏|Œ∏_t,X)$
      - Is called _maximization step_.
   3. If $Œ∏$ converged, then break the loop.

[mle-wiki]: https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Properties

$Œ∏_{t+1}$ consists of the following parameters:
- $p(z|Œ∏_{t+1}) := \sum_x {\color{orange}p(x,z|Œ∏^\star_t)}$
  - ${\color{orange}p(x,z|Œ∏^\star_t)} := {\color{orange}p(z|x,Œ∏_t)}/|X|$
- $\mu_{z|Œ∏_{t+1}} := \sum_x p(x|z,Œ∏_{t+1}) \cdot x$
  - $p(x|z,Œ∏_{t+1}) := {\color{orange}p(x,z|Œ∏^\star_t)} / p(z|Œ∏_{t+1})$
- $\sigma_{z|Œ∏_{t+1}} := \sqrt{ \sum_x p(x|z,Œ∏_{t+1}) \cdot (x-\mu_{z|Œ∏_{t+1}})^2 }$

Proof:

- $p(z|Œ∏_{t+1})$
  - $:= \arg\max_{p(z|Œ∏)} L(Œ∏|Œ∏_t,X)$
  - $\frac \partial {\partial p(z|Œ∏)} L(Œ∏|Œ∏_t,X) = 0$
    - $= \frac \partial {\partial p(z|Œ∏)}( L(Œ∏|Œ∏_t,X) - ùúÜ \cdot (\sum_z p(z|Œ∏)-1))$
      - $ùúÜ$ is the _Lagrange multiplier_.
    - $= \frac \partial {\partial p(z|Œ∏)} L(Œ∏|Œ∏_t,X) - \frac \partial {\partial p(z|Œ∏)} ùúÜ \cdot (\sum_z p(z|Œ∏)-1)$
    - $= \frac \partial {\partial p(z|Œ∏)} L(Œ∏|Œ∏_t,X) - ùúÜ$
    - $= \sum_x {\color{orange}p(x,z|Œ∏^\star_t)}/p(z|Œ∏) - ùúÜ$
  - $\sum_x {\color{orange}p(x,z|Œ∏^\star_t)}/p(z|Œ∏) - ùúÜ = 0$
  - $ùúÜ = \sum_x {\color{orange}p(x,z|Œ∏^\star_t)}/p(z|Œ∏)$
  - $ùúÜ \cdot p(z|Œ∏) = \sum_x {\color{orange}p(x,z|Œ∏^\star_t)}$
  - $\sum_z ùúÜ \cdot p(z|Œ∏) = \sum_z \sum_x {\color{orange}p(x,z|Œ∏^\star_t)}$
  - $ùúÜ \cdot \sum_z p(z|Œ∏) = \sum_x \sum_z {\color{orange}p(z|x,Œ∏_t)}/|X|$
  - $ùúÜ \cdot 1 = \sum_x 1/|X|$
  - $ùúÜ = 1$
  - $\sum_x {\color{orange}p(x,z|Œ∏^\star_t)}/p(z|Œ∏) - 1 = 0$
  - $1 = \sum_x {\color{orange}p(x,z|Œ∏^\star_t)}/p(z|Œ∏)$
  - $p(z|Œ∏) = \sum_x {\color{orange}p(x,z|Œ∏^\star_t)}$
  - $= \sum_x {\color{orange}p(x,z|Œ∏^\star_t)}$
- $p(x|z,Œ∏_{t+1})$
  - $:= \arg\max_{p(x|z,Œ∏)} L(Œ∏|Œ∏_t,X)$
  - $\frac \partial {\partial p(x|z,Œ∏)} L(Œ∏|Œ∏_t,X) = 0$
    - $= \frac \partial {\partial p(x|z,Œ∏)}( L(Œ∏|Œ∏_t,X) - ùúÜ \cdot (\sum_z p(x|z,Œ∏)-1))$
      - $ùúÜ$ is the _Lagrange multiplier_.
    - $= {\color{orange}p(x,z|Œ∏^\star_t)}/p(x|z,Œ∏) - ùúÜ$
  - ${\color{orange}p(x,z|Œ∏^\star_t)}/p(x|z,Œ∏) - ùúÜ = 0$
  - $ùúÜ = {\color{orange}p(x,z|Œ∏^\star_t)}/p(x|z,Œ∏)$
  - $ùúÜ \cdot p(x|z,Œ∏) = {\color{orange}p(x,z|Œ∏^\star_t)}$
  - $\sum_x ùúÜ \cdot p(x|z,Œ∏) = \sum_x {\color{orange}p(x,z|Œ∏^\star_t)}$
  - $ùúÜ \cdot \sum_x p(x|z,Œ∏) = p(z|Œ∏)$
  - $ùúÜ \cdot 1 = p(z|Œ∏)$
  - $ùúÜ = p(z|Œ∏)$
  - ${\color{orange}p(x,z|Œ∏^\star_t)} / p(x|z,Œ∏) - p(z|Œ∏) = 0$
  - $p(x|z,Œ∏) = {\color{orange}p(x,z|Œ∏^\star_t)} / p(z|Œ∏)$
  - $= {\color{orange}p(x,z|Œ∏^\star_t)} / p(z|Œ∏_{t+1})$
- $\mu_{z|Œ∏_{t+1}}$
  - $:= \arg\max_{\mu_{z|Œ∏}} L(Œ∏|Œ∏_t,X)$
  - $\frac \partial {\partial \mu_{z|Œ∏}} L(Œ∏|Œ∏_t,X) = 0$
    - $= \sum_x \frac \partial {\partial p(x|z,Œ∏)} L(Œ∏|Œ∏_t,X) \cdot \frac {\partial p(x|z,Œ∏)} {\partial \mu_{z|Œ∏}}$
  - $\frac \partial {\partial p(x|z,Œ∏)} L(Œ∏|Œ∏_t,X)$
    - $= {\color{orange}p(x,z|Œ∏^\star_t)}/p(x|z,Œ∏)$
  - $\frac {\partial p(x|z,Œ∏)} {\partial \mu_{z|Œ∏}}$
    - $= \frac 1 {\sigma_{z|Œ∏} {\sqrt{2\pi}}} e^{- \frac {(x-\mu_{z|Œ∏})^2} {2 \sigma_{z|Œ∏}^2}} \cdot (-\frac 1 {2\sigma_{z|Œ∏}^2} \cdot 2 \cdot (x-\mu_{z|Œ∏}) \cdot (-1))$
    - $= p(x|z,Œ∏) \cdot (x-\mu_{z|Œ∏})/\sigma_{z|Œ∏}^2$
  - $\sum_x {\color{orange}p(x,z|Œ∏^\star_t)} \cdot (x-\mu_{z|Œ∏})/\sigma_{z|Œ∏}^2 = 0$
  - $\sum_x {\color{orange}p(x,z|Œ∏^\star_t)} \cdot (x-\mu_{z|Œ∏}) = 0$
  - $\sum_x {\color{orange}p(x,z|Œ∏^\star_t)} \cdot \mu_{z|Œ∏} = \sum_x {\color{orange}p(x,z|Œ∏^\star_t)} \cdot x$
  - $\mu_{z|Œ∏} \cdot \sum_x {\color{orange}p(x,z|Œ∏^\star_t)} = \sum_x {\color{orange}p(x,z|Œ∏^\star_t)} \cdot x$
  - $\mu_{z|Œ∏} \cdot p(z|Œ∏) = \sum_x {\color{orange}p(x,z|Œ∏^\star_t)} \cdot x$
  - $\mu_{z|Œ∏} = \sum_x {\color{orange}p(x,z|Œ∏^\star_t)} / p(z|Œ∏) \cdot x$
  - $\mu_{z|Œ∏} = \sum_x p(x|z,Œ∏) \cdot x$
  - $= \sum_x p(x|z,Œ∏_{t+1}) \cdot x$
- $\sigma_{z|Œ∏_{t+1}}$
  - $:= \arg\max_{\sigma_{z|Œ∏}} L(Œ∏|Œ∏_t,X)$
  - $\frac \partial {\partial \sigma_{z|Œ∏}} L(Œ∏|Œ∏_t,X) = 0$
    - $= \sum_x \frac \partial {\partial p(x|z,Œ∏)} L(Œ∏|Œ∏_t,X) \cdot \frac {\partial p(x|z,Œ∏)} {\partial \sigma_{z|Œ∏}}$
  - $\frac {\partial p(x|z,Œ∏)} {\partial \sigma_{z|Œ∏}}$
    - $= \frac 1 {\sigma_{z|Œ∏} {\sqrt{2\pi}}} e^{- \frac {(x-\mu_{z|Œ∏})^2} {2 \sigma_{z|Œ∏}^2}} \cdot \frac {-(x-\mu_{z|Œ∏})^2} 2 \frac {-2} {\sigma_{z|Œ∏}^3} - \frac 1 {\sigma_{z|Œ∏} {\sqrt{2\pi}}} e^{- \frac {(x-\mu_{z|Œ∏})^2} {2 \sigma_{z|Œ∏}^2}} \cdot \frac 1 \sigma_{z|Œ∏}$
    - $= p(x|z,Œ∏) \cdot (x-\mu_{z|Œ∏})^2\sigma_{z|Œ∏}^{-3} - p(x|z,Œ∏) \cdot \sigma_{z|Œ∏}^{-1}$
    - $= p(x|z,Œ∏) \cdot \sigma_{z|Œ∏}^{-3} \left( (x-\mu_{z|Œ∏})^2 - \sigma_{z|Œ∏}^2 \right)$
  - $\sum_x {\color{orange}p(x,z|Œ∏^\star_t)} \cdot \sigma_{z|Œ∏}^{-3} \left( (x-\mu_{z|Œ∏})^2 - \sigma_{z|Œ∏}^2 \right) = 0$
  - $\sum_x {\color{orange}p(x,z|Œ∏^\star_t)} \cdot \left( (x-\mu_{z|Œ∏})^2 - \sigma_{z|Œ∏}^2 \right) = 0$
  - $\sum_x {\color{orange}p(x,z|Œ∏^\star_t)} \cdot \sigma_{z|Œ∏}^2 = \sum_x {\color{orange}p(x,z|Œ∏^\star_t)} \cdot (x-\mu_{z|Œ∏})^2$
  - $\sigma_{z|Œ∏}^2 \cdot \sum_x {\color{orange}p(x,z|Œ∏^\star_t)} = \sum_x {\color{orange}p(x,z|Œ∏^\star_t)} \cdot (x-\mu_{z|Œ∏})^2$
  - $\sigma_{z|Œ∏}^2 \cdot p(z|Œ∏) = \sum_x {\color{orange}p(x,z|Œ∏^\star_t)} \cdot (x-\mu_{z|Œ∏})^2$
  - $\sigma_{z|Œ∏}^2 = \sum_x {\color{orange}p(x,z|Œ∏^\star_t)} / p(z|Œ∏) \cdot (x-\mu_{z|Œ∏})^2$
  - $\sigma_{z|Œ∏}^2 = \sum_x p(x|z,Œ∏) \cdot (x-\mu_{z|Œ∏})^2$
  - $= \sqrt{ \sum_x p(x|z,Œ∏_{t+1}) \cdot (x-\mu_{z|Œ∏_{t+1}})^2 }$
